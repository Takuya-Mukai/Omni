{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnUTObth9EXH"
   },
   "source": [
    "# 第3回講義 演習  \n",
    "\n",
    "今回は，深層モデルやそのライブラリは用いず，多層パーセプトロンを実装します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt7ibMdOYDDb"
   },
   "source": [
    "## 目次\n",
    "\n",
    "1. [【課題 1】多層パーセプトロンの実装と学習(XOR)](#scrollTo=GGnLlc7ACgxf)\n",
    "\n",
    "    1.1. [活性化関数とその微分](#scrollTo=Kxzqey199-33)\n",
    "    \n",
    "    1.2. [データセットの設定と重みの定義](#scrollTo=9X-jAIqoCWsp)\n",
    "    \n",
    "    1.3. [train関数とvalid関数](#scrollTo=ZcG-GIvyDYXe)\n",
    "    \n",
    "    1.4. [学習](#scrollTo=Ep9LqYJtPl_s)\n",
    "\n",
    "1. [【課題 2】多層パーセプトロンの実装と学習(MNIST)](#scrollTo=fdEpBD--P8fD)\n",
    "\n",
    "    2.1. [ソフトマックス関数](#scrollTo=UDUGHs8TfXH2)\n",
    "    \n",
    "    2.2. [データセットの設定](#scrollTo=vTArTuMYgYDk)\n",
    "    \n",
    "    2.3. [全結合層の定義](#scrollTo=UQ75UXddhar_)\n",
    "    \n",
    "    2.4. [train関数とvalid関数](#scrollTo=mK7lR2Q-lc5K)\n",
    "    \n",
    "    2.5. [学習](#scrollTo=n_O-NCslmW3p)\n",
    "\n",
    "    2.6. [Tips:実験の可視化](#scrollTo=fKEU70W8cS4i)\n",
    "\n",
    "1. [【課題 3】数値微分（勾配チェック）](#scrollTo=WA98nAv1mxWu)\n",
    "\n",
    "    3.1. [1変数の場合](#scrollTo=cTFnh6oxofw2)\n",
    "\n",
    "    3.2. [多変数の場合(MLP)](#scrollTo=wCqJgtmipLrA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgWU0L0D9Mp-"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGnLlc7ACgxf"
   },
   "source": [
    "## 1.【課題 1】多層パーセプトロンの実装と学習(XOR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kxzqey199-33"
   },
   "source": [
    "### 1.1. 活性化関数とその微分\n",
    "まずは活性化関数の定義と，勾配の計算に利用する導関数を定義していきます．ここではシグモイド関数，ReLU関数，tanh関数を実装していきます．\n",
    "\n",
    "シグモイド関数は二値分類の出力層，ReLU関数とtanh関数は隠れ層の活性化関数として用いられることが多いですが，近年では勾配消失問題の対策としてReLU関数を利用するのが一般的です．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxl16kJQkLEX"
   },
   "source": [
    "**シグモイド関数**\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1+\\text{exp}(-x)} \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma'(x) = \\sigma(x)(1-\\sigma(x)) \\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QPWWfCj-v_J"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # 単純な実装\n",
    "    # return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # expのoverflow対策を施した実装\n",
    "    # x >=0 のとき sigmoid(x) = 1 / (1 + exp(-x))\n",
    "    # x < 0 のとき sigmoid(x) = exp(x) / (1 + exp(x))\n",
    "    return np.exp(np.minimum(x, 0)) / (1 + np.exp(- np.abs(x)))\n",
    "\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BxK6w5L_iGH"
   },
   "source": [
    "**ReLU関数**\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ReLU}(x) = \\text{max}(0, x) \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ReLU}'(x) =  \\begin{cases}\n",
    "    1 \\quad \\text{if} \\quad x > 0 \\tag{4} \\\\\n",
    "    0 \\quad \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pxa4PUmbBq3O"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def deriv_relu(x):\n",
    "    return (x > 0).astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if9Fmb3UByDO"
   },
   "source": [
    "**tanh関数**\n",
    "\\begin{equation}\n",
    "    \\text{tanh}(x) = \\frac{\\text{exp}(2x)-1}{\\text{exp}(2x)+1} \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{tanh}'(x) = 1 - \\text{tanh}^2(x) \\tag{6}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3MBPh5rCPrH"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X-jAIqoCWsp"
   },
   "source": [
    "### 1.2. データセットの設定と重みの定義  \n",
    "次にMLPを学習するためのデータセットと，パラメータの初期化を行います．\n",
    "\n",
    "まずはデータセットを作成します．\n",
    "\n",
    "データセットは非線形問題として知られるXOR問題を用います．講義でも扱いましたが，XOR問題は2つの入力$(x_1, x_2), \\quad x_1, x_2 \\in \\{0, 1\\}$を与え，2つの値が同じときは0，異なるときは1を割り当てます．これは二次元空間に描画した時に0に割り当てられる点と1に割り当てられる点を1つの直線で分類することができないため，非線形問題となっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AX0WH1j6x6f"
   },
   "outputs": [],
   "source": [
    "# XORデータセット\n",
    "x_train_xor = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "t_train_xor = np.array([[1], [1], [0], [0]])  # 正解ラベルをtとする\n",
    "x_valid_xor, t_valid_xor = x_train_xor, t_train_xor\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hlines([0], xmin=-1, xmax=2, color=\"black\", alpha=0.7)\n",
    "plt.vlines([0], ymin=-1, ymax=2, color=\"black\", alpha=0.7)\n",
    "plt.scatter(x_train_xor[0:2, 0], x_train_xor[0:2, 1], color=\"red\", label=\"1\")\n",
    "plt.scatter(x_train_xor[2:, 0], x_train_xor[2:, 1], color=\"blue\", label=\"0\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO_FNY4N7O0v"
   },
   "source": [
    "次にパラメータを初期化します．重みは一様分布からのサンプリング，バイアスは0で初期化を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrTd126iCcRv"
   },
   "outputs": [],
   "source": [
    "# 重み（入力層の次元数: 2，隠れ層の次元数: 8，出力層の次元数: 1）\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(2, 8)).astype(\"float64\")\n",
    "b1 = np.zeros(8).astype(\"float64\")\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(8, 1)).astype(\"float64\")\n",
    "b2 = np.zeros(1).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcG-GIvyDYXe"
   },
   "source": [
    "### 1.3. train関数とvalid関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmpRvsU_DkVI"
   },
   "source": [
    "隠れ層と出力層の2層からなるMLPを実装していきます．\n",
    "\n",
    "なお今回は，予測ラベルを$y$，正解ラベルを$t$，学習率を$ε$\n",
    "とします（今後の演習でもこの表記を使用することがあります）．\n",
    "\n",
    "**目的関数**\n",
    "\n",
    "負の対数尤度（2クラス交差エントロピー）\n",
    "\\begin{equation}\n",
    "E(\\mathbf{x}, \\mathbf{t}) = - \\frac{1}{N} \\sum^N_{i=1}\\left[ \\mathbf{t}_i \\log{\\mathbf{y}_i} + (1 - \\mathbf{t}_i) \\log{(1 - \\mathbf{y_i})}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**順伝播**\n",
    "\\begin{align}\n",
    "\\mathbf{u}^{1} &= (\\mathbf{W}^{1})^{T} \\mathbf{x} + \\mathbf{b}^{1} \\tag{隠れ層} \\\\\n",
    "\\mathbf{h}^{1} &= \\text{ReLU}(\\mathbf{u}^{1}) \\tag{隠れ層} \\\\\n",
    "\\mathbf{u}^{2} &= (\\mathbf{W}^{2})^{T} \\mathbf{h}^{1} + \\mathbf{b}^{2} \\tag{出力層} \\\\\n",
    "\\mathbf{y} &= \\sigma(\\mathbf{u}^{2}) \\tag{出力層}\n",
    "\\end{align}\n",
    "\n",
    "**逆伝播**\n",
    "\\begin{align}\n",
    "\\delta^{2} &= \\mathbf{y} - \\mathbf{t} \\tag{出力層} \\\\\n",
    "\\delta^{1} &= \\text{ReLU}'(\\mathbf{u}^{1}) \\odot ((\\mathbf{W}^{2})^{T} \\delta^{2}) \\tag{隠れ層}\n",
    "\\end{align}\n",
    "\n",
    "**勾配の計算**\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{W}^{1}}E &= \\frac{1}{N}\\delta^{1}\\mathbf{x}^T \\tag{隠れ層} \\\\\n",
    "\\nabla_{\\mathbf{b}^{1}}E &= \\frac{1}{N}\\delta^{1}\\mathbb{1}_N \\tag{隠れ層} \\\\\n",
    "\\nabla_{\\mathbf{W}^{2}}E &= \\frac{1}{N}\\delta^{2}(\\mathbf{h}^{1})^{T} \\tag{出力層} \\\\\n",
    "\\nabla_{\\mathbf{b}^{2}}E &= \\frac{1}{N}\\delta^{2}\\mathbb{1}_N \\tag{出力層}\n",
    "\\end{align}\n",
    "\n",
    "**重みの更新**\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{1} \\leftarrow \\mathbf{W}^{1} - \\epsilon \\nabla_{\\mathbf{W}^{1}} E \\tag{隠れ層} \\\\\n",
    "\\mathbf{b}^{1} \\leftarrow \\mathbf{b}^{1} - \\epsilon \\nabla_{\\mathbf{b}^{1}} E \\tag{隠れ層} \\\\\n",
    "\\mathbf{W}^{2} \\leftarrow \\mathbf{W}^{2} - \\epsilon \\nabla_{\\mathbf{W}^{2}} E \\tag{出力層} \\\\\n",
    "\\mathbf{b}^{2} \\leftarrow \\mathbf{b}^{2} - \\epsilon \\nabla_{\\mathbf{b}^{2}} E \\tag{出力層}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvI5FXhbPOF0"
   },
   "outputs": [],
   "source": [
    "# logの中身が0になることを防ぐ\n",
    "def np_log(x):\n",
    "    return np.log(np.clip(x, 1e-10, 1e+10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOYS37oDPVEM"
   },
   "outputs": [],
   "source": [
    "def train_xor(x, t, eps):\n",
    "    \"\"\"\n",
    "    :param x: np.ndarray, 入力データ, (batch_size, 入力層の次元数)\n",
    "    :param t: np.ndarray, 教師ラベル, (batch_size, 出力層の次元数)\n",
    "    :param eps: float, 学習率\n",
    "    \"\"\"\n",
    "    global W1, b1, W2, b2\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    # 順伝播\n",
    "    u1 = np.matmul(x, W1) + b1  # (batch_size, 隠れ層の次元数)\n",
    "    h1 = relu(u1)\n",
    "\n",
    "    u2 = np.matmul(h1, W2) + b2  # (batch_size, 出力層の次元数)\n",
    "    y = sigmoid(u2)\n",
    "\n",
    "    # 誤差の計算\n",
    "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
    "\n",
    "    # 逆伝播\n",
    "    delta_2 =  # WRITE ME # (batch_size, 出力層の次元数)\n",
    "    delta_1 =  # WRITE ME # (batch_size, 隠れ層の次元数)\n",
    "\n",
    "    # 勾配の計算\n",
    "    dW1 =  # WRITE ME # (入力層の次元数, 隠れ層の次元数)\n",
    "    db1 =  # WRITE ME # (隠れ層の次元数,)\n",
    "\n",
    "    dW2 =  # WRITE ME # (隠れ層の次元数, 出力層の次元数)\n",
    "    db2 =  # WRITE ME # (出力層の次元数,)\n",
    "\n",
    "    # パラメータの更新\n",
    "    W1 -=  # WRITE ME\n",
    "    b1 -=  # WRITE ME\n",
    "\n",
    "    W2 -=  # WRITE ME\n",
    "    b2 -=  # WRITE ME\n",
    "\n",
    "    return cost\n",
    "\n",
    "def valid_xor(x, t):\n",
    "    global W1, b1, W2, b2\n",
    "\n",
    "    # 順伝播\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    h1 = relu(u1)\n",
    "\n",
    "    u2 = np.matmul(h1, W2) + b2\n",
    "    y = sigmoid(u2)\n",
    "\n",
    "    # 誤差の計算\n",
    "    cost =  # WRITE ME\n",
    "\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep9LqYJtPl_s"
   },
   "source": [
    "### 1.4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeIR5sbZPoh6"
   },
   "outputs": [],
   "source": [
    "for epoch in range(3000):\n",
    "    # オンライン学習\n",
    "    for x, t in zip(x_train_xor, t_train_xor):\n",
    "        cost = train_xor(x[None, :], t[None, :], eps=0.05)\n",
    "\n",
    "cost, y_pred = valid_xor(x_valid_xor, t_valid_xor)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdEpBD--P8fD"
   },
   "source": [
    "## 2.【課題 2】多層パーセプトロンの実装と学習(MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDUGHs8TfXH2"
   },
   "source": [
    "### 2.1. ソフトマックス関数\n",
    "ソフトマックス関数は多クラス分類の出力層で用いられる活性化関数です．こちらも勾配の計算に利用するために導関数も実装していきます．  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{softmax}(\\mathbf{x})_k = \\frac{\\text{exp}(\\mathbf{x}_k)}{\\sum^K_{k'=1} \\text{exp}(\\mathbf{x}_{k'})} \\quad \\quad \\text{for} \\quad k=1, \\dots K\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{softmax}'(\\mathbf{x})_k = \\text{softmax}(\\mathbf{x})_k (1 - \\text{softmax}(\\mathbf{x}))_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cubfAAegHIm"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x -= x.max(axis=1, keepdims=True)  # オーバーフローを避ける\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTArTuMYgYDk"
   },
   "source": [
    "### 2.2. データセットの設定\n",
    "次にデータセットを作成します．ここでは第2回の演習でも利用したMNISTデータセットを用います．データセット・データの前処理に関する説明については第2回の演習をご参考ください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLJBpp9EgdIP"
   },
   "outputs": [],
   "source": [
    "(x_mnist_1, t_mnist_1), (x_mnist_2, t_mnist_2) = mnist.load_data()\n",
    "\n",
    "x_mnist = np.r_[x_mnist_1, x_mnist_2]\n",
    "t_mnist = np.r_[t_mnist_1, t_mnist_2]\n",
    "\n",
    "x_mnist = x_mnist.astype(\"float64\") / 255.  # 値を[0, 1]に正規化する\n",
    "t_mnist = np.eye(N=10)[t_mnist.astype(\"int32\").flatten()]  # one-hotベクトルにする\n",
    "\n",
    "x_mnist = x_mnist.reshape(x_mnist.shape[0], -1)  # 1次元に変換\n",
    "\n",
    "# train data: 5000, valid data: 10000, test data: 10000にする\n",
    "x_train_mnist, x_test_mnist, t_train_mnist, t_test_mnist =\\\n",
    "    train_test_split(x_mnist, t_mnist, test_size=10000)\n",
    "x_train_mnist, x_valid_mnist, t_train_mnist, t_valid_mnist =\\\n",
    "    train_test_split(x_train_mnist, t_train_mnist, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ75UXddhar_"
   },
   "source": [
    "### 2.3. 全結合層の定義  \n",
    "\n",
    "多層のMLPを実装できるように，全結合層をクラスとして定義します．\n",
    "\n",
    "順伝播，逆伝播，勾配の計算をそれぞれ関数として実装します．\n",
    "\n",
    "数式は以下のようになります．  \n",
    "\n",
    "**順伝播**(`__call__`)\n",
    "\\begin{align}\n",
    "\\mathbf{u}^{l} &= (\\mathbf{W}^{l})^{T}\\mathbf{h}^{l-1} + \\mathbf{b}^{l}  \\\\\n",
    "\\mathbf{h}^{l} &= \\text{function}(\\mathbf{u}^{l})\n",
    "\\end{align}\n",
    "\n",
    "**逆伝播**(`b_prop`)\n",
    "\\begin{equation}\n",
    "\\delta^{l} = \\text{function}'(\\mathbf{u}^{l}) \\odot ((\\mathbf{W}^{l+1})^{T} \\delta^{l+1})\n",
    "\\end{equation}\n",
    "\n",
    "**勾配の計算**(`compute_grad`)\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{W}^{l}}E &= \\frac{1}{N}\\delta^{l}(\\mathbf{h}^{l-1})^{T} \\\\\n",
    "\\nabla_{\\mathbf{b}^{l}}E &= \\frac{1}{N}\\delta^{l}\\mathbb{1}_N\n",
    "\\end{align}\n",
    "\n",
    "`__call__`は，インスタンスを関数のように呼び出すための特殊メソッドです．\n",
    "```python\n",
    "Class A:\n",
    "  def __init__(self):\n",
    "    ...\n",
    "  def __call__(self):\n",
    "    print('Hello World.')\n",
    "\n",
    "a = A()\n",
    "a()  # __call__が呼び出される\n",
    "# Hello world.\n",
    "```\n",
    "\n",
    "`get_params`，`set_params`，`get_grads`はそれぞれ重み，勾配をベクトルで受け渡す関数です．課題3の勾配チェックの際に使用します．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuAPucgki9wF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function, deriv_function):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08,\n",
    "                                   size=(in_dim, out_dim)).astype(\"float64\")\n",
    "        self.b = np.zeros(out_dim).astype(\"float64\")\n",
    "        self.function = function\n",
    "        self.deriv_function = deriv_function\n",
    "\n",
    "        self.x = None\n",
    "        self.u = None\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "        self.params_idxs = np.cumsum([self.W.size, self.b.size])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        順伝播処理を行うメソッド．\n",
    "        x: (batch_size, in_dim_{j})\n",
    "        h: (batch_size, out_dim_{j})\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.u = np.matmul(self.x, self.W) + self.b\n",
    "        h = self.function(self.u)\n",
    "        return h\n",
    "\n",
    "    def b_prop(self, delta, W):\n",
    "        \"\"\"\n",
    "        誤差逆伝播を行うメソッド．\n",
    "        delta (=delta_{j+1}): (batch_size, out_dim_{j+1})\n",
    "        W (=W_{j+1}): (out_dim_{j}, out_dim_{j+1})\n",
    "        self.delta (=delta_{j}): (batch_size, out_dim_{j})\n",
    "        \"\"\"\n",
    "        self.delta =  self.deriv_function(self.u) * np.matmul(delta, W.T)\n",
    "        return self.delta\n",
    "\n",
    "    def compute_grad(self):\n",
    "        \"\"\"\n",
    "        勾配を計算するメソッド．\n",
    "        self.x: (batch_size, in_dim_{j})\n",
    "        self.delta: (batch_size, out_dim_{j})\n",
    "        self.dW: (in_dim_{j}, out_dim_{j})\n",
    "        self.db: (out_dim_{j})\n",
    "        \"\"\"\n",
    "        batch_size = self.delta.shape[0]\n",
    "\n",
    "        self.dW =  # WRITE ME\n",
    "        self.db =  # WRITE ME\n",
    "\n",
    "    def get_params(self):\n",
    "        return np.concatenate([self.W.ravel(), self.b], axis=0)\n",
    "\n",
    "    def set_params(self, params):\n",
    "        \"\"\"\n",
    "        params: List[np.ndarray, np.ndarray]\n",
    "            1つ目の要素が重みW: (in_dim, out_dim)，2つ目の要素がバイアス: (out_dim,)\n",
    "        \"\"\"\n",
    "        _W, _b = np.split(params, self.params_idxs)[:-1]\n",
    "        self.W = _W.reshape(self.W.shape)\n",
    "        self.b = _b\n",
    "\n",
    "    def get_grads(self):\n",
    "        return np.concatenate([self.dW.ravel(), self.db], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kpeZlEGCaTF"
   },
   "source": [
    "このDenseクラスを用いてモデル全体をModelクラスとして実装します．今後利用する深層学習ライブラリのPyTorchでも似たようなモデルの定義をするので今のうちに慣れておきましょう．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqt5BqilCo1W"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, hidden_dims, activation_functions, deriv_functions):\n",
    "        \"\"\"\n",
    "        :param hiden_dims: List[int]，各層のノード数を格納したリスト．\n",
    "        :params activation_functions: List, 各層で用いる活性化関数を格納したリスト．\n",
    "        :params derive_functions: List，各層で用いる活性化関数の導関数を格納したリスト．\n",
    "        \"\"\"\n",
    "        # 各層をリストに格納していく\n",
    "        self.layers = []\n",
    "        for i in range(len(hidden_dims)-2):  # 出力層以外は同じ構造\n",
    "            self.layers.append(Dense(hidden_dims[i], hidden_dims[i+1],\n",
    "                                     activation_functions[i], deriv_functions[i]))\n",
    "        self.layers.append(Dense(hidden_dims[-2], hidden_dims[-1],\n",
    "                                 activation_functions[-1], deriv_functions[-1]))  # 出力層を追加\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播処理を行うメソッド\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"誤差逆伝播，勾配計算を行うメソッド\"\"\"\n",
    "        batch_size = delta.shape[0]\n",
    "\n",
    "        for i, layer in enumerate(self.layers[::-1]):\n",
    "            if i == 0:  # 出力層の場合\n",
    "                layer.delta = delta  # y - t\n",
    "                layer.compute_grad()\n",
    "            else:  # 出力層以外の場合\n",
    "                delta = layer.b_prop(delta, W)  # 逆伝播\n",
    "                layer.compute_grad()  # 勾配の計算\n",
    "\n",
    "            W = layer.W\n",
    "\n",
    "    def update(self, eps=0.01):\n",
    "        \"\"\"パラメータの更新を行うメソッド\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.W -= eps * layer.dW\n",
    "            layer.b -= eps * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izjg7ZSEEyoi"
   },
   "source": [
    "実際にモデルを利用するときはこのクラスのインスタンスを作成します．本課題では3層のMLPを用います．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0VnNqrzE7Iw"
   },
   "outputs": [],
   "source": [
    "model = Model(hidden_dims=[784, 100, 100, 10],\n",
    "              activation_functions=[relu, relu, softmax],\n",
    "              deriv_functions=[deriv_relu, deriv_relu, deriv_softmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FjIR6qZIL1F"
   },
   "source": [
    "課題1ではデータを1つずつ渡すオンライン学習を用いましたが，ここではミニバッチ学習を用います．そのためにデータセットをミニバッチに分割する関数を定義します．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwgtWxdCIMpq"
   },
   "outputs": [],
   "source": [
    "def create_batch(data, batch_size):\n",
    "    \"\"\"\n",
    "    :param data: np.ndarray，入力データ\n",
    "    :param batch_size: int，バッチサイズ\n",
    "    \"\"\"\n",
    "    num_batches, mod = divmod(data.shape[0], batch_size)\n",
    "    batched_data = np.split(data[: batch_size * num_batches], num_batches)\n",
    "    if mod:\n",
    "        batched_data.append(data[batch_size * num_batches:])\n",
    "\n",
    "    return batched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK7lR2Q-lc5K"
   },
   "source": [
    "### 2.4. train関数とvalid関数\n",
    "\n",
    "**誤差関数**  \n",
    "\n",
    "負の対数尤度（多クラス交差エントロピー）\n",
    "\\begin{equation}\n",
    "E(\\mathbf{x}, \\mathbf{t}) = - \\frac{1}{N} \\sum^N_{i=1} \\sum^K_{k=1}\\mathbf{t}_{i, k} \\log{\\mathbf{y}_{i, k}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWNxa_1SJoBJ"
   },
   "source": [
    "上記の損失関数と，先に定義したModelクラスのインスタンスを用いて，モデルを訓練するための関数を定義します．この関数では1エポック分の訓練を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi_r6wsBl2gI"
   },
   "outputs": [],
   "source": [
    "def train_mst(model, x, t, eps=0.01):\n",
    "    # 順伝播\n",
    "    y = model(x)\n",
    "\n",
    "    # 誤差の計算\n",
    "    cost = (-t * np_log(y)).sum(axis=1).mean()\n",
    "\n",
    "    # 逆伝播\n",
    "    delta = y - t\n",
    "    model.backward(delta)\n",
    "\n",
    "    # パラメータの更新\n",
    "    model.update(eps)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-5n5Jg5JyNx"
   },
   "source": [
    "同様に訓練したモデルを評価するための関数を定義します．関数の中身は訓練とほとんど変わりませんが，評価時には誤差逆伝播による勾配の計算と，パラメータの更新が不要になります．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfiV2dfPmHPI"
   },
   "outputs": [],
   "source": [
    "def valid_mst(model, x, t):\n",
    "    # 順伝播\n",
    "    y = model(x)\n",
    "\n",
    "    # 誤差の計算\n",
    "    cost = (-t * np_log(y)).sum(axis=1).mean()\n",
    "\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_O-NCslmW3p"
   },
   "source": [
    "### 2.5. 学習  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZdTe6oJmYT1"
   },
   "outputs": [],
   "source": [
    "# バッチサイズを指定\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(10):\n",
    "    x_train_mnist, t_train_mnist = shuffle(x_train_mnist, t_train_mnist)\n",
    "    x_train_batch, t_train_batch = \\\n",
    "        create_batch(x_train_mnist, batch_size), create_batch(t_train_mnist, batch_size)\n",
    "    # ミニバッチ学習\n",
    "    for x, t in zip(x_train_batch, t_train_batch):\n",
    "        cost = train_mst(model, x, t, eps=0.1)\n",
    "\n",
    "    cost, y_pred = valid_mst(model, x_valid_mnist, t_valid_mnist)\n",
    "    accuracy = accuracy_score(t_valid_mnist.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print(f\"EPOCH: {epoch+1} Valid Cost: {cost:.3f} Valid Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKEU70W8cS4i"
   },
   "source": [
    "### 2.6.Tips:実験の可視化\n",
    "\n",
    "通常，lossやaccuracyなどのログは，数値だけで追うのはわかりにくいため，グラフで可視化して確認します．\n",
    "\n",
    "その際によく使われるツールとして，[Weights & Biases](https://wandb.ai/site/ja/)と[Tensorboard](https://www.tensorflow.org/tensorboard?hl=ja)が挙げられます．\n",
    "\n",
    "\n",
    "- Weights & Biases: クラウドベースの実験管理ツールで，非常に多機能です．可視化だけでなく，複数の実験の比較やハイパーパラメータの自動チューニング，モデルのチェックポイント保存なども可能です．利用するにはアカウント登録が必要です．\n",
    "- Tensorboard: TensorFlow公式の可視化ツールで，ローカル環境で簡単に使うことができます．\n",
    "\n",
    "宿題や最終課題に取り組む際に積極的に活用してみてください．\n",
    "\n",
    "Google Colabでの使い方は下記を参照してください．\n",
    "\n",
    "- [Weights & Biasesのチュートリアル](https://docs.wandb.ai/ja/tutorials)\n",
    "- [Tensorboardのチュートリアル（TensorFlow）](https://www.tensorflow.org/tensorboard/get_started?hl=ja)\n",
    "- [TensorboardをPytorchで使う方法](https://qiita.com/go50/items/ae5f979b9bb36bfbb6be)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA98nAv1mxWu"
   },
   "source": [
    "## 3.【課題 3】数値微分（勾配チェック）  \n",
    "\n",
    "誤差逆伝播法による勾配の計算は少し複雑なため，実装にバグが入りがちです．\n",
    "\n",
    "実装が簡単な数値微分と結果を比較することで，逆伝播の実装が正しいかを確認してみましょう．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTFnh6oxofw2"
   },
   "source": [
    "### 3.1. 1変数の場合\n",
    "\n",
    "まず簡単な2次関数に対して数値微分を行ってみましょう．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZm8yb1UomkR"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "def deriv_f(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSbMYmfyoqKp"
   },
   "source": [
    "1変数の場合の数値微分の式は以下のようになります．  \n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) = \\underset{h \\rightarrow 0}{\\text{lim}} \\frac{f(x + h) - f(x - h)}{2h}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv9JGSOKpAXA"
   },
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "x = 2.0\n",
    "\n",
    "grad_auto = deriv_f(x)\n",
    "grad_num = (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "print(grad_auto, grad_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCqJgtmipLrA"
   },
   "source": [
    "### 3.2. 多変数の場合(MLP)\n",
    "次に課題2で定義したMLPに対して数値微分の計算を行い，誤差逆伝播による勾配(`dW`, `db`)の計算が間違っていないかを確認してみましょう．\n",
    "\n",
    "多変数(MLP)の場合の数値微分の式は次のようになります．ある1つの変数$\\theta_m$のみを$h$だけ動かした場合を考えます．  \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial \\theta_m} = \\underset{h \\rightarrow 0}{\\text{lim}}\\frac{E(\\theta_1, \\theta_2, \\dots , \\theta_m + h, \\dots , \\theta_M) - E(\\theta_1, \\theta_2, \\dots , \\theta_m - h, \\dots , \\theta_M)}{2h}\n",
    "\\end{equation}\n",
    "\n",
    "実装では，変数全体のサイズのゼロベクトルを用意し，$m$番目の要素のみ$h$だけずらされたベクトルを作り，それに対応する誤差を計算し，そこから上の式に従って最終的な微分の値を求めていきます．  \n",
    "\n",
    "まず各層ごとの重みをベクトルで取得しリストで保存していく関数を実装していきます．  \n",
    "\n",
    "MLPの各レイヤーから重みをベクトルで取得するために，先ほど`Dense`クラスで定義した`set_params`，`get_params`を使用します．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0lpoIlsqJfw"
   },
   "outputs": [],
   "source": [
    "def get_params(layers):\n",
    "    params_all = []\n",
    "    for layer in layers:\n",
    "        params = layer.get_params()\n",
    "        params_all.append(params)\n",
    "\n",
    "    return params_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nehSAkCtqRBA"
   },
   "outputs": [],
   "source": [
    "def set_params(laeyrs, params_all):\n",
    "    for layer, params in zip(model.layers, params_all):\n",
    "        layer.set_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LT1fV2aSqVx4"
   },
   "outputs": [],
   "source": [
    "def compute_cost(x, t):\n",
    "    # 順伝播\n",
    "    y = model(x)\n",
    "\n",
    "    # 誤差の計算\n",
    "    cost = (-t * np_log(y)).sum(axis=1).mean()\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtc0XoQaqhyR"
   },
   "source": [
    "#### 3.2.1. 数値微分\n",
    "\n",
    "勾配の計算に使用するデータを用意します．勾配チェックの際はどれだけ精度よく近似できているかを見たいので，`float64`を使いましょう．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVYwUiUvqwww"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "x = x_train_mnist[:batch_size].astype(\"float64\")\n",
    "t = t_train_mnist[:batch_size].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4TvumnLq20Z"
   },
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "params_all = get_params(model.layers)\n",
    "grads_all_num = []\n",
    "\n",
    "# レイヤーごとに勾配を計算\n",
    "for layer, params in zip(model.layers, params_all):\n",
    "    shift = np.zeros_like(params)\n",
    "    grads_num = np.zeros_like(params)\n",
    "\n",
    "    # レイヤー内のM個のパラメータに対してそれぞれ微分を計算する\n",
    "    for m in range(len(params)):\n",
    "        shift[m] = eps  # m番目のパラメータのみeps分ずらす [0, 0, ..., 0, eps, 0, ..., 0]\n",
    "\n",
    "        params_right = params + shift\n",
    "        layer.set_params(params_right)\n",
    "        cost_right = compute_cost(x, t)  # L(x; ..., \\theta_m + eps, ...)\n",
    "\n",
    "        params_left = params - shift\n",
    "        layer.set_params(params_left)\n",
    "        cost_left = compute_cost(x, t)  # L(x; ..., \\theta_m - eps, ...)\n",
    "\n",
    "        grads_num[m] = (cost_right - cost_left) / (2 * eps)  # 微分の計算\n",
    "\n",
    "        layer.set_params(params)\n",
    "        shift[m] = 0\n",
    "\n",
    "    grads_all_num.append(grads_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njAOv63or0pI"
   },
   "source": [
    "#### 3.2.2. 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHEBk04asHeh"
   },
   "outputs": [],
   "source": [
    "def get_grads(layers):\n",
    "    grads_all = []\n",
    "    for layer in layers:\n",
    "        grads = layer.get_grads()\n",
    "        grads_all.append(grads)\n",
    "\n",
    "    return grads_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSiADsqesORg"
   },
   "outputs": [],
   "source": [
    "# 順伝播\n",
    "y = model(x)\n",
    "\n",
    "# 逆伝播\n",
    "delta = y - t\n",
    "model.backward(delta)\n",
    "\n",
    "# 勾配を計算\n",
    "grads_all_bprop = get_grads(model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF2AUFdtsW4g"
   },
   "source": [
    "#### 3.2.3. 比較（勾配チェック）\n",
    "\n",
    "誤差逆伝播法で計算した勾配と数値微分による勾配の差を，ノルムで正規化したrelative differenceで測ります．経験的にはその差がおおよそ1e-7以下であれば実装にバグはないと安心していいでしょう．\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{diff} = \\frac{||\\text{grad}_{\\text{bprop}} - \\text{grad}_{\\text{num}}||_2}{||\\text{grad}_{\\text{bprop}}||_2 + ||\\text{grad}_{\\text{num}}||_2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Reference: Improving Deep Neural Networks: Gradient checking https://www.coursera.org/lecture/deep-neural-network/gradient-checking-htA0l (2018年10月17日閲覧)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5cbNK-xtKxx"
   },
   "outputs": [],
   "source": [
    "for i, (grads_bprop, grads_num) in enumerate(zip(grads_all_bprop, grads_all_num)):\n",
    "    diff = np.linalg.norm(grads_bprop - grads_num) / (np.linalg.norm(grads_bprop) + np.linalg.norm(grads_num))\n",
    "    print(f\"Gradients' difference (layer {i+1}: {diff})\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
